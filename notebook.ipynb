{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8678fc73-9602-42cf-b9cc-9da88e9836aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\python3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:\\\\Datasets\\\\datasets\\\\gauravduttakiit\\\\resume-dataset\\\\versions\\\\1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "os.environ['KAGGLEHUB_CACHE'] = 'E:\\\\Datasets'\n",
    "kagglehub.dataset_download(\"gauravduttakiit/resume-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7291ba-985b-45e5-9bc2-1ccdd9a9f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717823c4-ee4c-42db-8664-8d03f675b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97678747-7fb5-47b3-99da-c7e6e10fd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690021a3-f4c9-4cbb-8805-6e25df0e297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\\\Datasets\\\\datasets\\\\gauravduttakiit\\\\resume-dataset\\\\versions\\\\1\\\\UpdatedResumeDataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9314f06-e5cd-4b1c-a532-1e1bac4f819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                             Resume\n",
       "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
       "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
       "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
       "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
       "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "536a6921-6cfa-4464-b8a6-c5b6d826beff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>962</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Java Developer</td>\n",
       "      <td>Technical Skills Web Technologies: Angular JS,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category                                             Resume\n",
       "count              962                                                962\n",
       "unique              25                                                166\n",
       "top     Java Developer  Technical Skills Web Technologies: Angular JS,...\n",
       "freq                84                                                 18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbc77ea-971e-4bc6-ab97-c4442b942513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Exprience - 24 months\\r\\njQuery- Exprience - 24 months\\r\\nPython- Exprience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIEW\\r\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\\r\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to review all red flag cases.\\r\\nâ\\x80¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0b07559-e37f-470e-92ed-2f8c620bdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52a3a33-fbd0-4616-b73e-afe808819e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cv(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0902cf60-9887-4191-9b18-3629711c2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Resume'] = df['Resume'].apply(clean_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05d3c89-1b46-4d21-b30e-a1d910a2232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "      <th>Cleaned_Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>skills * programming languages: python (pandas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "      <td>education details may 2013 to may 2017 b.e uit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "      <td>areas of interest deep learning, control syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
       "      <td>skills r python sap hana tableau sap hana sql ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "      <td>education details mca ymcaust, faridabad, hary...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                             Resume  \\\n",
       "0  Data Science  Skills * Programming Languages: Python (pandas...   \n",
       "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...   \n",
       "2  Data Science  Areas of Interest Deep Learning, Control Syste...   \n",
       "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...   \n",
       "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...   \n",
       "\n",
       "                                      Cleaned_Resume  \n",
       "0  skills * programming languages: python (pandas...  \n",
       "1  education details may 2013 to may 2017 b.e uit...  \n",
       "2  areas of interest deep learning, control syste...  \n",
       "3  skills r python sap hana tableau sap hana sql ...  \n",
       "4  education details mca ymcaust, faridabad, hary...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d969aef-e0a7-47cf-a6e6-2e9fe5bbcdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skills * programming languages: python (pandas, numpy, scipy, scikit-learn, matplotlib), sql, java, javascript/jquery. * machine learning: regression, svm, na ve bayes, knn, random forest, decision trees, boosting techniques, cluster analysis, word embedding, sentiment analysis, natural language processing, dimensionality reduction, topic modelling (lda, nmf), pca & neural nets. * database visualizations: mysql, sqlserver, cassandra, hbase, elasticsearch d3.js, dc.js, plotly, kibana, matplotlib, ggplot, tableau. * others: regular expression, html, css, angular 6, logstash, kafka, python flask, git, docker, computer vision - open cv and understanding of deep learning.education details data science assurance associate data science assurance associate - ernst & young llp skill details javascript- exprience - 24 months jquery- exprience - 24 months python- exprience - 24 monthscompany details company - ernst & young llp description - fraud investigations and dispute services assurance technology assisted review tar (technology assisted review) assists in accelerating the review process and run analytics and generate reports. * core member of a team helped in developing automated review platform tool from scratch for assisting e discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review. * understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. worked on analyzing the outputs and precision monitoring for the entire tool. * tar assists in predictive coding, topic modelling from the evidence by following ey standards. developed the classifier models in order to identify \"red flags\" and fraud-related issues. tools & technologies: python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, na ve bayes, lda, nmf for topic modelling, vader and text blob for sentiment analysis. matplot lib, tableau dashboard for reporting. multiple data science and analytic projects (usa clients) text analytics - motor vehicle customer review data * received customer feedback survey data for past one year. performed sentiment (positive, negative & neutral) and time series analysis on customer comments across all 4 categories. * created heat map of terms by survey category based on frequency of words * extracted positive and negative words across all the survey categories and plotted word cloud. * created customized tableau dashboards for effective reporting and visualizations. chatbot * developed a user friendly chatbot for one of our products which handle simple questions about hours of operation, reservation options and so on. * this chat bot serves entire product related questions. giving overview of tool via qa platform and also give recommendation responses so that user question to build chain of relevant answer. * this too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions. tools & technologies: python, natural language processing, nltk, spacy, topic modelling, sentiment analysis, word embedding, scikit-learn, javascript/jquery, sqlserver information governance organizations to make informed decisions about all of the information they store. the integrated information governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk. * scan data from multiple sources of formats and parse different file formats, extract meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana. * preforming rot analysis on the data which give information of data which helps identify content that is either redundant, outdated, or trivial. * preforming full-text search analysis on elastic search with predefined methods which can tag as (pii) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks. tools & technologies: python, flask, elastic search, kibana fraud analytic platform fraud analytics and investigative platform to review all red flag cases. fap is a fraud analytics and investigative platform with inbuilt case manager and suite of analytics for various erp systems. * it can be used by clients to interrogate their accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics tools & technologies: html, javascript, sqlserver, jquery, css, bootstrap, node.js, d3.js, dc.js'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2426befa-8b90-402d-a288-94648292216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# model_id = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dc9cc56-7399-4202-bb97-85191f571bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_prompt(resume_text):\n",
    "    return f\"\"\"\n",
    "You are a structured resume parsing agent.\n",
    "\n",
    "## INSTRUCTIONS:\n",
    "Extract the following fields from the resume. Respond only with a valid JSON object, using double quotes. Do not guess.\n",
    "\n",
    "- \"experience_years\": Float. Sum of relevant work experience in years. Use months/12 if stated. If unclear, use 0.0.\n",
    "- \"technical_skills\": List (max 10). Tools, languages, or frameworks. All lowercase.\n",
    "- \"soft_skills\": List (max 3). Interpersonal or cognitive skills. No technical terms. All lowercase.\n",
    "- \"education_level\": One of [\"bachelor\", \"master\", \"phd\", \"other\"].\n",
    "- \"fit_score\": Float [0.0–5.0], based on:\n",
    "  - experience (0–2)\n",
    "  - technical skill breadth (0–2)\n",
    "  - soft skills (0–1)\n",
    "\n",
    "## EXAMPLE RESUME:\n",
    "education: b.e in computer engineering, 2017. skills: python, java, sql, scikit-learn. worked 18 months as data analyst at ABC corp. known for team collaboration and adaptability.\n",
    "\n",
    "## EXAMPLE OUTPUT:\n",
    "{{\n",
    "  \"experience_years\": 1.5,\n",
    "  \"technical_skills\": [\"python\", \"java\", \"sql\", \"scikit-learn\"],\n",
    "  \"soft_skills\": [\"collaboration\", \"adaptability\"],\n",
    "  \"education_level\": \"bachelor\",\n",
    "  \"fit_score\": 3.7\n",
    "}}\n",
    "\n",
    "## EXAMPLE RESUME:\n",
    "education: masters in cs, 2014. skills: python, excel, databricks, scikit-learn. worked 24 months as data engineer. team player.\n",
    "\n",
    "## EXAMPLE OUTPUT:\n",
    "{{\n",
    "  \"experience_years\": 2.0,\n",
    "  \"technical_skills\": [\"python\", \"excel\", \"databricks\", \"scikit-learn\"],\n",
    "  \"soft_skills\": [\"team\", \"collaboration\"],\n",
    "  \"education_level\": \"master\",\n",
    "  \"fit_score\": 4.1\n",
    "}}\n",
    "\n",
    "## EXAMPLE RESUME:\n",
    "education: undergraduated cs student, 2019. skills: python, pytorch, keras, tensorflow. internship for 3 months at school. i am proactive, agile, quick-learner student.\n",
    "\n",
    "## EXAMPLE OUTPUT:\n",
    "{{\n",
    "  \"experience_years\": 0.3,\n",
    "  \"technical_skills\": [\"python\", \"pytorch\", \"keras\", \"tensorflow\"],\n",
    "  \"soft_skills\": [\"proactive\", \"agile\", \"quick learner\"],\n",
    "  \"education_level\": \"other\",\n",
    "  \"fit_score\": 3.5\n",
    "}}\n",
    "\n",
    "### Now analyze the resume below. Don't copy any example. Parse it precisely.\n",
    "## RESUME:\n",
    "\\\"\\\"\\\"\n",
    "{resume_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "FINISH ON JSON CLOSING BRACKET \"}}\"\n",
    "## OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a02b793a-3050-436d-aecb-637385f9e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_zero_shot_prompt(resume_text):\n",
    "    return f\"\"\"\n",
    "You are a structured resume parsing agent.\n",
    "\n",
    "## INSTRUCTIONS:\n",
    "Extract the following fields from the resume. Respond only with a valid JSON object, using double quotes. Do not guess.\n",
    "\n",
    "- \"experience_years\": Float. Sum of relevant work experience in years. Use months/12 if stated. If unclear, use 0.0.\n",
    "- \"technical_skills\": List (max 10). Tools, languages, or frameworks. All lowercase.\n",
    "- \"soft_skills\": List (max 3). Interpersonal or cognitive skills. No technical terms. All lowercase.\n",
    "- \"education_level\": One of [\"bachelor\", \"master\", \"phd\", \"other\"].\n",
    "- \"fit_score\": Float [0.0–5.0], based on:\n",
    "  - experience (0–2)\n",
    "  - technical skill breadth (0–2)\n",
    "  - soft skills (0–1)\n",
    "\n",
    "### Now analyze the resume below. Don't copy any example. Parse it precisely.\n",
    "## RESUME:\n",
    "\\\"\\\"\\\"\n",
    "{resume_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "FINISH ON JSON CLOSING BRACKET \"}}\"\n",
    "## OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ca060ae-2c34-42cf-b2dc-91640e717af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"skills r python sap hana tableau sap hana sql sap hana pal ms sql sap lumira c# linear programming data modelling advance analytics scm analytics retail analytics social media analytics nlp education details january 2017 to january 2018 pgdm business analytics great lakes institute of management & illinois institute of technology january 2013 bachelor of engineering electronics and communication bengaluru, karnataka new horizon college of engineering, bangalore visvesvaraya technological university data science consultant consultant - deloitte usi skill details linear programming- exprience - 6 months retail- exprience - 6 months retail marketing- exprience - 6 months scm- exprience - 6 months sql- exprience - less than 1 year months deep learning- exprience - less than 1 year months machine learning- exprience - less than 1 year months python- exprience - less than 1 year months r- exprience - less than 1 year monthscompany details company - deloitte usi description - the project involved analysing historic deals and coming with insights to optimize future deals. role: was given raw data, carried out end to end analysis and presented insights to client. key responsibilities: extract data from client systems across geographies. understand and build reports in tableau. infer meaningful insights to optimize prices and find out process blockades. technical environment: r, tableau. industry: cross industry service area: cross industry - products project name: handwriting recognition consultant: 3 months. the project involved taking handwritten images and converting them to digital text images by object detection and sentence creation. role: i was developing sentence correction functionality. key responsibilities: gather data large enough to capture all english words train lstm models on words. technical environment: python. industry: finance service area: financial services - bi development project name: swift consultant: 8 months. the project was to develop an analytics infrastructure on top of sap s/4, it would user to view financial reports to respective departments. reporting also included forecasting expenses. role: i was leading the offshore team. key responsibilities: design & develop data models for reporting. develop etl for data flow validate various reports. technical environment: sap hana, tableau, sap ao. industry: healthcare analytics service area: life sciences - product development project name: clinical healthcare system consultant: 2 months. the project was to develop an analytics infrastructure on top of argus, it would allow users to query faster and provide advance analytics capabilities. role: i was involved from design to deploy phase, performed a lot of data restructuring and built models for insights. key responsibilities: design & develop data models for reporting. develop and deploy analytical models. validate various reports. technical environment: data modelling, sap hana, tableau, nlp. industry: fmcg service area: trade & promotion project name: consumption based planning for flowers foods consultant; 8 months. the project involved setting up of crm and cbp modules. role: i was involved in key data decomposition activities and setting up the base for future year forecast. over the course of the project i developed various models and carried out key performance improvements. key responsibilities: design & develop hana models for decomposition. develop data flow for forecast. developed various views for reporting of customer/sales/funds. validate various reports in bobj. technical environment: data modelling, sap hana, bobj, time series forecasting. internal initiative industry: fmcg customer segmentation and rfm analysis consultant; 3 months. the initiative involved setting up of hana-python interface and advance analytics on python. over the course i had successfully segmented data into five core segments using k-means and carried out rfm analysis in python. also developed algorithm to categorize any new customer under the defined buckets. technical environment: anaconda3, python3.6, hana sps12 industry: telecom invoice state detection consultant; 1 months. the initiative was to reduce the manual effort in verifying closed and open invoices manually, it involved development to a decision tree to classify open/closed invoices. this enabled effort reduction by 60%. technical environment: r, sap pal, sap hana sps12 accenture experience industry: analytics - cross industry in process analytics for sap senior developer; 19 months. accenture solutions pvt. ltd., india the project involved development of sap analytics tool - in process analytics (ipa) . my role was to develop database objects and data models to provide operational insights to clients. role: i have developed various finance related kpis and spearheaded various deployments. introduced sap predictive analytics to reduce development time and reuse functionalities for kpis and prepared production planning reports. key responsibilities: involved in information gather phase. designed and implemented sap hana data modelling using attribute view, analytic view, and calculation view. developed various kpi's individually using complex sql scripts in calculation views. created procedures in hana database. took ownership and developed dashboard functionality. involved in building data processing algorithms to be executed in r server for cluster analysis. technical environment: r, sap hana, t-sql. industry: cross industry accenture testing accelerator for sap database developer; 21 months. accenture solutions pvt. ltd., india role: i have taken care of all development activities for the atas tool and have also completed various deployments of the product. apart from these activities i was also actively involved in maintenance of the database servers (production & quality) key responsibilities: analyzing business requirements, understanding the scope, getting requirements clarified interacting with business and further transform all requirements to generate attribute mapping documents and reviewing mapping specification documentation create / update database objects like tables, views, stored procedures, function, and packages monitored sql server error logs and application logs through sql server agent prepared data flow diagrams, entity relationship diagrams using uml responsible for designing, developing and normalization of database tables experience in performance tuning using sql profiler. involved in qa, uat, knowledge transfer and support activities technical environment: sql server 2008/2014, visual studio 2010, windows server, performance monitor, sql server profiler, c#, pl-sql, t-sql.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_Resume'].iloc[3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92fa125d-122d-48ef-a76f-d718a0d41bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a structured resume parsing agent.\n",
      "\n",
      "## INSTRUCTIONS:\n",
      "Extract the following fields from the resume. Respond only with a valid JSON object, using double quotes. Do not guess.\n",
      "\n",
      "- \"experience_years\": Float. Sum of relevant work experience in years. Use months/12 if stated. If unclear, use 0.0.\n",
      "- \"technical_skills\": List (max 10). Tools, languages, or frameworks. All lowercase.\n",
      "- \"soft_skills\": List (max 3). Interpersonal or cognitive skills. No technical terms. All lowercase.\n",
      "- \"education_level\": One of [\"bachelor\", \"master\", \"phd\", \"other\"].\n",
      "- \"fit_score\": Float [0.0–5.0], based on:\n",
      "  - experience (0–2)\n",
      "  - technical skill breadth (0–2)\n",
      "  - soft skills (0–1)\n",
      "\n",
      "### Now analyze the resume below. Don't copy any example. Parse it precisely.\n",
      "## RESUME:\n",
      "\"\"\"\n",
      "education details may 2013 to may 2017 b.e uit-rgpv data scientist data scientist - matelabs skill details python- exprience - less than 1 year months statsmodels- exprience - 12 months aws- exprience - less than 1 year months machine learning- exprience - less than 1 year months sklearn- exprience - less than 1 year months scipy- exprience - less than 1 year months keras- exprience - less than 1 year monthscompany details company - matelabs description - ml platform for business professionals, dummies and enthusiasts. 60/a koramangala 5th block, achievements/tasks behind sukh sagar, bengaluru, india developed and deployed auto preprocessing steps of machine learning mainly missing value treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction. deployed automated classification and regression model. linkedin.com/in/aditya-rathore- b4600b146 reasearch and deployed the time series forecasting model arima, sarimax, holt-winter and prophet. worked on meta-feature extracting problem. github.com/rathorology implemented a state of the art research paper on outlier detection for mixed attributes. company - matelabs description -\n",
      "\"\"\"\n",
      "\n",
      "FINISH ON JSON CLOSING BRACKET \"}\"\n",
      "## OUTPUT:\n",
      "{\n",
      "  \"experience_years\": 2.0,\n",
      "  \"technical_skills\": [\n",
      "    \"python\",\n",
      "    \"statsmodels\",\n",
      "    \"aws\",\n",
      "    \"machine learning\",\n",
      "    \"sklearn\",\n",
      "    \"scipy\",\n",
      "    \"keras\"\n",
      "  ],\n",
      "  \"soft_skills\": [\n",
      "    \"ml platform for business professionals\",\n",
      "    \"dummies and enthusiasts\",\n",
      "    \"auto preprocessing steps of machine learning\",\n",
      "    \"outlier detection\",\n",
      "    \"encoding\",\n",
      "    \"scaling\",\n",
      "    \"feature selection and dimensionality reduction\",\n",
      "    \"auto preprocessing steps of machine learning mainly missing value treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction\"\n",
      "  ],\n",
      "  \"education_level\": \"bachelor\",\n",
      "  \"fit_score\": 3.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = build_zero_shot_prompt(df['Cleaned_Resume'].iloc[11,])\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "output = pipe(prompt, max_new_tokens=500, temperature=0.4, top_p=0.8)[0][\"generated_text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82b11126-da16-4964-8170-adb440431bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1: temperature=0.2, top_p=0.7 ---\n",
      "ERROR! 'NoneType' object has no attribute 'group'\n",
      "{'experience_years': 52.0, 'technical_skills': ['python', 'math', 'numpy', 'pandas', 'scikit-learn'], 'soft_skills': ['data science', 'machine learning'], 'education_level': 'bachelor', 'fit_score': 3.163397726092923, 'favorite_location': 'India'}\n",
      "\n",
      "--- Test 2: temperature=0.4, top_p=0.8 ---\n",
      "ERROR! 'NoneType' object has no attribute 'group'\n",
      "{'experience_years': 52.0, 'technical_skills': ['python', 'math', 'numpy', 'pandas', 'scikit-learn'], 'soft_skills': ['data science', 'machine learning'], 'education_level': 'bachelor', 'fit_score': 3.163397726092923, 'favorite_location': 'India'}\n",
      "\n",
      "--- Test 3: temperature=0.6, top_p=0.9 ---\n",
      "{'experience_years': 4, 'technical_skills': ['python', 'machine learning', 'data science'], 'soft_skills': ['data science', 'machine learning', 'data analysis'], 'education_level': 'bachelor', 'fit_score': 3.0}\n",
      "\n",
      "--- Test 4: temperature=0.8, top_p=1.0 ---\n",
      "{'experience_years': 4.0, 'technical_skills': ['python', 'numpy', 'scipy', 'matplotlib'], 'soft_skills': ['data science', 'machine learning'], 'education_level': 'bachelor', 'fit_score': 5.0, 'skills_list': ['python', 'numpy', 'scipy', 'matplotlib', 'sklearn'], 'company': 'matelabs', 'job_title': 'data scientist'}\n",
      "\n",
      "--- Test 5: temperature=1.0, top_p=0.95 ---\n",
      "{'experience_years': 2.0, 'technical_skills': ['Python', 'Matplotlib', 'Scipy'], 'soft_skills': ['Keras', 'Azure'], 'fit_score': 4.0}\n",
      "\n",
      "--- Test 6: temperature=0.2, top_p=0.3 ---\n",
      "{'experience_years': 12, 'technical_skills': ['python', 'statsmodels', 'aws', 'machine learning', 'sklearn', 'scipy', 'keras', 'matelabs'], 'soft_skills': ['ml platform for business professionals', 'dummies and enthusiasts', 'auto preprocessing steps', 'missing value treatment', 'outlier detection', 'encoding', 'scaling', 'feature selection', 'dimensionality reduction', 'auto preprocessing steps of machine learning mainly missing value treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction'], 'education_level': 'bachelor', 'fit_score': 0.5}\n",
      "\n",
      "--- Test 7: temperature=0.3, top_p=0.45 ---\n",
      "{'experience_years': 12, 'technical_skills': ['python', 'statsmodels', 'aws', 'machine learning', 'sklearn', 'scipy', 'keras', 'matelabs'], 'soft_skills': ['ml platform for business professionals', 'dummies and enthusiasts', 'auto preprocessing steps', 'missing value treatment', 'outlier detection', 'encoding', 'scaling', 'feature selection', 'dimensionality reduction', 'auto preprocessing steps of machine learning mainly missing value treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction'], 'education_level': 'bachelor', 'fit_score': 3.0}\n"
     ]
    }
   ],
   "source": [
    "param_pairs = [\n",
    "    (0.2, 0.7),\n",
    "    (0.4, 0.8),\n",
    "    (0.6, 0.9),\n",
    "    (0.8, 1.0),\n",
    "    (1.0, 0.95),\n",
    "    (0.2, 0.3),\n",
    "    (0.3, 0.45)\n",
    "]\n",
    "pattern = r\"## OUTPUT:\\s*(\\{.*\\})\"\n",
    "prompt = build_zero_shot_prompt(df['Cleaned_Resume'].iloc[11,])\n",
    "\n",
    "for idx, (temp, top_p) in enumerate(param_pairs):\n",
    "    print(f\"\\n--- Test {idx+1}: temperature={temp}, top_p={top_p} ---\")\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=500,\n",
    "        temperature=temp,\n",
    "        top_p=top_p\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    match = re.search(pattern, output, re.DOTALL)\n",
    "    try:\n",
    "        json_output = match.group(1).strip()\n",
    "        json_string = json.loads(json_output)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR! {e}\")\n",
    "    print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1b682-29a8-46a5-8d89-1b66d8da75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1: temperature=0.2, top_p=0.7 ---\n"
     ]
    }
   ],
   "source": [
    "param_pairs = [\n",
    "    (0.2, 0.7),\n",
    "    (0.4, 0.8),\n",
    "    (0.6, 0.9),\n",
    "    (0.8, 1.0),\n",
    "    (1.0, 0.95),\n",
    "    (0.2, 0.3),\n",
    "    (0.3, 0.45)\n",
    "]\n",
    "pattern = r\"## OUTPUT:\\s*(\\{.*\\})\"\n",
    "prompt = build_few_shot_prompt(df['Cleaned_Resume'].iloc[11,])\n",
    "\n",
    "for idx, (temp, top_p) in enumerate(param_pairs):\n",
    "    print(f\"\\n--- Test {idx+1}: temperature={temp}, top_p={top_p} ---\")\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=500,\n",
    "        temperature=temp,\n",
    "        top_p=top_p\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    match = re.search(pattern, output, re.DOTALL)\n",
    "    try:\n",
    "        json_output = match.group(1).strip()\n",
    "        json_string = json.loads(json_output)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR! {e}\")\n",
    "    print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ba268d-baec-4b93-8e74-a1b4bb134ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"## OUTPUT:\\s*(\\{.*\\})\"\n",
    "match = re.search(pattern, output, re.DOTALL)\n",
    "try:\n",
    "    json_output = match.group(1).strip()\n",
    "    json_string = json.loads(json_output)\n",
    "except:\n",
    "    print(\"ERROR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fadfb8c-3920-4e2a-ba20-011a0e0c568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5,\n",
       " ['python', 'core java', 'database management'],\n",
       " ['collaboration', 'adaptability'],\n",
       " 'bachelor',\n",
       " 3.7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(json_string.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28ccfd1b-00e4-4226-926a-8463f6dc2373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience_years',\n",
       " 'technical_skills',\n",
       " 'soft_skills',\n",
       " 'education_level',\n",
       " 'fit_score']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(json_string.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d039beb5-0e4a-4626-b48a-8b99b14d0c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Science',\n",
       " 'Education Details \\r\\nMay 2013 to May 2017 B.E   UIT-RGPV\\r\\nData Scientist \\r\\n\\r\\nData Scientist - Matelabs\\r\\nSkill Details \\r\\nPython- Exprience - Less than 1 year months\\r\\nStatsmodels- Exprience - 12 months\\r\\nAWS- Exprience - Less than 1 year months\\r\\nMachine learning- Exprience - Less than 1 year months\\r\\nSklearn- Exprience - Less than 1 year months\\r\\nScipy- Exprience - Less than 1 year months\\r\\nKeras- Exprience - Less than 1 year monthsCompany Details \\r\\ncompany - Matelabs\\r\\ndescription - ML Platform for business professionals, dummies and enthusiasts.\\r\\n60/A Koramangala 5th block,\\r\\nAchievements/Tasks behind sukh sagar, Bengaluru,\\r\\nIndia                               Developed and deployed auto preprocessing steps of machine learning mainly missing value\\r\\ntreatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.\\r\\nDeployed automated classification and regression model.\\r\\nlinkedin.com/in/aditya-rathore-\\r\\nb4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and\\r\\nProphet.\\r\\nWorked on meta-feature extracting problem.\\r\\ngithub.com/rathorology\\r\\nImplemented a state of the art research paper on outlier detection for mixed attributes.\\r\\ncompany - Matelabs\\r\\ndescription - ',\n",
       " 'education details may 2013 to may 2017 b.e uit-rgpv data scientist data scientist - matelabs skill details python- exprience - less than 1 year months statsmodels- exprience - 12 months aws- exprience - less than 1 year months machine learning- exprience - less than 1 year months sklearn- exprience - less than 1 year months scipy- exprience - less than 1 year months keras- exprience - less than 1 year monthscompany details company - matelabs description - ml platform for business professionals, dummies and enthusiasts. 60/a koramangala 5th block, achievements/tasks behind sukh sagar, bengaluru, india developed and deployed auto preprocessing steps of machine learning mainly missing value treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction. deployed automated classification and regression model. linkedin.com/in/aditya-rathore- b4600b146 reasearch and deployed the time series forecasting model arima, sarimax, holt-winter and prophet. worked on meta-feature extracting problem. github.com/rathorology implemented a state of the art research paper on outlier detection for mixed attributes. company - matelabs description -']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = df.iloc[1,]\n",
    "obj.iloc[2]\n",
    "\n",
    "row = [*obj.tolist()] \n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ea56d54-df1e-49a7-9b0c-23251644b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f132326-76dd-4658-bfe1-aff609781c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 'NoneType' object has no attribute 'group'\n",
      "\n",
      "You are a structured resume parsing agent.\n",
      "\n",
      "## INSTRUCTIONS:\n",
      "Extract the following fields from the resume. Respond only with a valid JSON object, using double quotes. Do not guess.\n",
      "\n",
      "- \"experience_years\": Float. Sum of relevant work experience in years. Use months/12 if stated. If unclear, use 0.0.\n",
      "- \"technical_skills\": List (max 10). Tools, languages, or frameworks. All lowercase.\n",
      "- \"soft_skills\": List (max 3). Interpersonal or cognitive skills. No technical terms. All lowercase.\n",
      "- \"education_level\": One of [\"bachelor\", \"master\", \"phd\", \"other\"].\n",
      "- \"fit_score\": Float [0.0–5.0], based on:\n",
      "  - experience (0–2)\n",
      "  - technical skill breadth (0–2)\n",
      "  - soft skills (0–1)\n",
      "\n",
      "## EXAMPLE RESUME:\n",
      "education: b.e in computer engineering, 2017. skills: python, java, sql, scikit-learn. worked 18 months as data analyst at ABC corp. known for team collaboration and adaptability.\n",
      "\n",
      "## EXAMPLE OUTPUT:\n",
      "{\n",
      "  \"experience_years\": 1.5,\n",
      "  \"technical_skills\": [\"python\", \"java\", \"sql\", \"scikit-learn\"],\n",
      "  \"soft_skills\": [\"collaboration\", \"adaptability\"],\n",
      "  \"education_level\": \"bachelor\",\n",
      "  \"fit_score\": 3.7\n",
      "}\n",
      "\n",
      "## RESUME:\n",
      "\"\"\"\n",
      "skills r python sap hana tableau sap hana sql sap hana pal ms sql sap lumira c# linear programming data modelling advance analytics scm analytics retail analytics social media analytics nlp education details january 2017 to january 2018 pgdm business analytics great lakes institute of management & illinois institute of technology january 2013 bachelor of engineering electronics and communication bengaluru, karnataka new horizon college of engineering, bangalore visvesvaraya technological university data science consultant consultant - deloitte usi skill details linear programming- exprience - 6 months retail- exprience - 6 months retail marketing- exprience - 6 months scm- exprience - 6 months sql- exprience - less than 1 year months deep learning- exprience - less than 1 year months machine learning- exprience - less than 1 year months python- exprience - less than 1 year months r- exprience - less than 1 year monthscompany details company - deloitte usi description - the project involved analysing historic deals and coming with insights to optimize future deals. role: was given raw data, carried out end to end analysis and presented insights to client. key responsibilities: extract data from client systems across geographies. understand and build reports in tableau. infer meaningful insights to optimize prices and find out process blockades. technical environment: r, tableau. industry: cross industry service area: cross industry - products project name: handwriting recognition consultant: 3 months. the project involved taking handwritten images and converting them to digital text images by object detection and sentence creation. role: i was developing sentence correction functionality. key responsibilities: gather data large enough to capture all english words train lstm models on words. technical environment: python. industry: finance service area: financial services - bi development project name: swift consultant: 8 months. the project was to develop an analytics infrastructure on top of sap s/4, it would user to view financial reports to respective departments. reporting also included forecasting expenses. role: i was leading the offshore team. key responsibilities: design & develop data models for reporting. develop etl for data flow validate various reports. technical environment: sap hana, tableau, sap ao. industry: healthcare analytics service area: life sciences - product development project name: clinical healthcare system consultant: 2 months. the project was to develop an analytics infrastructure on top of argus, it would allow users to query faster and provide advance analytics capabilities. role: i was involved from design to deploy phase, performed a lot of data restructuring and built models for insights. key responsibilities: design & develop data models for reporting. develop and deploy analytical models. validate various reports. technical environment: data modelling, sap hana, tableau, nlp. industry: fmcg service area: trade & promotion project name: consumption based planning for flowers foods consultant; 8 months. the project involved setting up of crm and cbp modules. role: i was involved in key data decomposition activities and setting up the base for future year forecast. over the course of the project i developed various models and carried out key performance improvements. key responsibilities: design & develop hana models for decomposition. develop data flow for forecast. developed various views for reporting of customer/sales/funds. validate various reports in bobj. technical environment: data modelling, sap hana, bobj, time series forecasting. internal initiative industry: fmcg customer segmentation and rfm analysis consultant; 3 months. the initiative involved setting up of hana-python interface and advance analytics on python. over the course i had successfully segmented data into five core segments using k-means and carried out rfm analysis in python. also developed algorithm to categorize any new customer under the defined buckets. technical environment: anaconda3, python3.6, hana sps12 industry: telecom invoice state detection consultant; 1 months. the initiative was to reduce the manual effort in verifying closed and open invoices manually, it involved development to a decision tree to classify open/closed invoices. this enabled effort reduction by 60%. technical environment: r, sap pal, sap hana sps12 accenture experience industry: analytics - cross industry in process analytics for sap senior developer; 19 months. accenture solutions pvt. ltd., india the project involved development of sap analytics tool - in process analytics (ipa) . my role was to develop database objects and data models to provide operational insights to clients. role: i have developed various finance related kpis and spearheaded various deployments. introduced sap predictive analytics to reduce development time and reuse functionalities for kpis and prepared production planning reports. key responsibilities: involved in information gather phase. designed and implemented sap hana data modelling using attribute view, analytic view, and calculation view. developed various kpi's individually using complex sql scripts in calculation views. created procedures in hana database. took ownership and developed dashboard functionality. involved in building data processing algorithms to be executed in r server for cluster analysis. technical environment: r, sap hana, t-sql. industry: cross industry accenture testing accelerator for sap database developer; 21 months. accenture solutions pvt. ltd., india role: i have taken care of all development activities for the atas tool and have also completed various deployments of the product. apart from these activities i was also actively involved in maintenance of the database servers (production & quality) key responsibilities: analyzing business requirements, understanding the scope, getting requirements clarified interacting with business and further transform all requirements to generate attribute mapping documents and reviewing mapping specification documentation create / update database objects like tables, views, stored procedures, function, and packages monitored sql server error logs and application logs through sql server agent prepared data flow diagrams, entity relationship diagrams using uml responsible for designing, developing and normalization of database tables experience in performance tuning using sql profiler. involved in qa, uat, knowledge transfer and support activities technical environment: sql server 2008/2014, visual studio 2010, windows server, performance monitor, sql server profiler, c#, pl-sql, t-sql.\n",
      "\"\"\"\n",
      "\n",
      "## OUTPUT:\n",
      "{\n",
      "  \"experience_years\": 1.5,\n",
      "  \"technical_skills\": [\"python\", \"java\", \"sql\", \"scikit-learn\"],\n",
      "  \"soft_skills\": [\"collaboration\", \"adaptability\"],\n",
      "  \"education_level\": \"bachelor\",\n",
      "  \"fit_score\": 3.7,\n",
      "  \"skills_r_python_sap_hana_tableau_sap_lumira_c#_linear_programming_data_modelling_advance_analytics_scm_analytics_retail_analytics_social_media_analytics_nlp_education_details_january_2017_to_january_2018_pgdm_business_analytics_great_lakes_institute_of_management_and_illinois_institute_of_technology_january_2013_bacelor_of_engineering_episode_1_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_00_0_0_0_0_0_0_0_0_0_0_0\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2120 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR Expecting ',' delimiter: line 9 column 6 (char 280)\n",
      "\n",
      "You are a structured resume parsing agent.\n",
      "\n",
      "## INSTRUCTIONS:\n",
      "Extract the following fields from the resume. Respond only with a valid JSON object, using double quotes. Do not guess.\n",
      "\n",
      "- \"experience_years\": Float. Sum of relevant work experience in years. Use months/12 if stated. If unclear, use 0.0.\n",
      "- \"technical_skills\": List (max 10). Tools, languages, or frameworks. All lowercase.\n",
      "- \"soft_skills\": List (max 3). Interpersonal or cognitive skills. No technical terms. All lowercase.\n",
      "- \"education_level\": One of [\"bachelor\", \"master\", \"phd\", \"other\"].\n",
      "- \"fit_score\": Float [0.0–5.0], based on:\n",
      "  - experience (0–2)\n",
      "  - technical skill breadth (0–2)\n",
      "  - soft skills (0–1)\n",
      "\n",
      "## EXAMPLE RESUME:\n",
      "education: b.e in computer engineering, 2017. skills: python, java, sql, scikit-learn. worked 18 months as data analyst at ABC corp. known for team collaboration and adaptability.\n",
      "\n",
      "## EXAMPLE OUTPUT:\n",
      "{\n",
      "  \"experience_years\": 1.5,\n",
      "  \"technical_skills\": [\"python\", \"java\", \"sql\", \"scikit-learn\"],\n",
      "  \"soft_skills\": [\"collaboration\", \"adaptability\"],\n",
      "  \"education_level\": \"bachelor\",\n",
      "  \"fit_score\": 3.7\n",
      "}\n",
      "\n",
      "## RESUME:\n",
      "\"\"\"\n",
      "education details b.tech rayat and bahra institute of engineering and biotechnology data science data science skill details numpy- exprience - less than 1 year months machine learning- exprience - less than 1 year months tensorflow- exprience - less than 1 year months scikit- exprience - less than 1 year months python- exprience - less than 1 year months gcp- exprience - less than 1 year months pandas- exprience - less than 1 year months neural network- exprience - less than 1 year monthscompany details company - wipro description - bhawana aggarwal e-mail:bhawana.chd@gmail.com phone: 09876971076 vversatile, high-energy professional targeting challenging assignments in machine profile summary an it professional with knowledge and experience of 2 years in wipro technologies in machine learning, deep learning, data science, python, software development. skilled in managing end-to-end development and software products / projects from inception, requirement specs, planning, designing, implementation, configuration and documentation. knowledge on python , machine learning, deep learning, data science, algorithms, neural network, nlp, gcp. knowledge on python libraries like numpy, pandas, seaborn , matplotlib, cufflinks. knowledge on different algorithms in machine learning like knn, decision tree, bias variance trade off, support vector machine(svm),logistic regression, neural networks. have knowledge on unsupervised, supervised and reinforcement data. programming experience in relational platforms like mysql,oracle. have knowledge on some programming language like c++,java. experience in cloud based environment like google cloud. working on different operating system like linux, ubuntu, windows. good interpersonal and communication skills. problem solving skills with the ability to think laterally, and to think with a medium term and long term perspective flexibility and an open attitude to change. ability to create, define and own frameworks with a strong emphasis on code reusability. technical skills programming languages python, c libraries seaborn, numpy, pandas, cufflinks, matplotlib algorithms knn, decision tree, linear regression, logistic regression, neural networks, k means clustering, tensorflow, svm databases sql, oracle operating systems linux, window development environments netbeans, notebooks, sublime ticketing tools service now, remedy education ug education: b.tech (computer science) from rayat and bahra institute of engineering and biotechnology passed with 78.4%in 2016. schooling: xii in 2012 from moti ram arya sr. secondary school(passed with 78.4%) x in 2010 from valley public school (passed with 9.4 cgpa) work experince title : wipro neural intelligence platform team size : 5 brief: wipro s neural intelligence platform harnesses the power of automation and artificial intelligence technologies natural language processing (nlp), cognitive, machine learning, and analytics. the platform comprises three layers: a data engagement platform that can easily access and manage multiple structured and unstructured data sources; an intent assessment and reasoning engine that includes sentiment and predictive analytics; and a deep machine learning engine that can sense, act, and learn over time. the project entailed automating responses to user queries at the earliest. the monster bot using the power of deep machine learning, nlp to handle such queries. user can see the how their queries can be answered quickly like alll1 activities can be eliminated. entity extractor -> this involves text extraction and nlp for fetching out important information from the text like dates, names, places, contact numbers etc. this involves regex, bluemix nlu api s and machine learning using tensor flow for further learning of new entities. classifier ->this involves the classifications of classes, training of dataset and predicting the output using the sklearn classifier (mnb, svm, sgd as classifier) and sgd for the optimization to map the user queries with the best suited response and make the system efficient. ner: a deep learning ner model is trained to extract the entities from the text. entities like roles, skills, organizations can be extracted from raw text. rnn(lstm) bidirectional model is trained for extracting such entities using keras tensorflow framework. other projects title : diabetes detection brief : developed the software which can detect whether the person is suffering from diabetes or not and got the third prize in it. training and certifications title: python training, machine learning, data science, deep learning organization: udemy, coursera (machine learning, deep learning) personal profile father s name :mr. tirlok aggarwal language known : english & hindi marital status :single date of birth(gender):1993-12-20(yyyy-mm-dd) (f) company - wipro description - developing programs in python. company - wipro description - title : wipro neural intelligence platform team size : 5 brief: wipro s neural intelligence platform harnesses the power of automation and artificial intelligence technologies natural language processing (nlp), cognitive, machine learning, and analytics. the platform comprises three layers: a data engagement platform that can easily access and manage multiple structured and unstructured data sources; an intent assessment and reasoning engine that includes sentiment and predictive analytics; and a deep machine learning engine that can sense, act, and learn over time. the project entailed automating responses to user queries at the earliest. the monster bot using the power of deep machine learning, nlp to handle such queries. user can see the how their queries can be answered quickly like alll1 activities can be eliminated. entity extractor -> this involves text extraction and nlp for fetching out important information from the text like dates, names, places, contact numbers etc. this involves regex, bluemix nlu api s and machine learning using tensor flow for further learning of new entities. classifier ->this involves the classifications of classes, training of dataset and predicting the output using the sklearn classifier (mnb, svm, sgd as classifier) and sgd for the optimization to map the user queries with the best suited response and make the system efficient. ner: a deep learning ner model is trained to extract the entities from the text. entities like roles, skills, organizations can be extracted from raw text. rnn(lstm) bidirectional model is trained for extracting such entities using keras tensorflow framework. company - wipro technologies description - an it professional with knowledge and experience of 2 years in wipro technologies in machine learning, deep learning, data science, python, software development. skilled in managing end-to-end development and software products / projects from inception, requirement specs, planning, designing, implementation, configuration and documentation. knowledge on python , machine learning, deep learning, data science, algorithms, neural network, nlp, gcp. knowledge on python libraries like numpy, pandas, seaborn , matplotlib, cufflinks. knowledge on different algorithms in machine learning like knn, decision tree, bias variance trade off, support vector machine(svm),logistic regression, neural networks. have knowledge on unsupervised, supervised and reinforcement data. programming experience in relational platforms like mysql,oracle. have knowledge on some programming language like c++,java. experience in cloud based environment like google cloud. working on different operating system like linux, ubuntu, windows. good interpersonal and communication skills. problem solving skills with the ability to think laterally, and to think with a medium term and long term perspective flexibility and an open attitude to change. ability to create, define and own frameworks with a strong emphasis on code reusability.\n",
      "\"\"\"\n",
      "\n",
      "## OUTPUT:\n",
      "{\n",
      "  \"education\": {\n",
      "    \"freshman\": {\n",
      "      \"years\": 2,\n",
      "      \"school\": \"rayat and bahra institute of engineering and biotechnology\",\n",
      "      \"college\": \"moti ram arya sr. Secondary school(passed with 78.4%\",\n",
      "      \"major\": \"computer science\",\n",
      "      \"major\": \"computer science\"\n",
      "    },\n",
      "    \"sophomore\": {\n",
      "      \"years: \"x rayat \"oracle\" \"java \"java \"sql \"sql \"mysql-backing \"00% \"c.b.b. Title:b. skills \"skills\" \"skill\" \"skills \"skills \"skills \"computation \"skills \"\n",
      "\n",
      "\"nervous \"skills \"skills \"python.\n",
      "\n",
      "\"python\"\n",
      "\n",
      "\"python \"mysql, \"b. Skills.\n",
      "python, resommes, \"python, andments.\n",
      "python: python, \"\n",
      "python:json:\n",
      "python:\n",
      "datab,1, \"python, \"python,skital:\n",
      "\n",
      "\n",
      "\n",
      "python:\n",
      "data, \"python:skents andders, andsks, andskints, andsks,reds,red, tok,ints,ners orsk, andredents, \"months, andredents, andps.\n",
      "contacts, orders, ors.\n",
      "or,s,0, or, or,ss,1s, ands, ors,s.\n",
      "sent,\n",
      "sent.\n",
      "or, and, and, and, orps.\n",
      "or,s,s, or, or,s,ss, and,ss,ss,ss, with aps:\n",
      "will,\n",
      "s,\n",
      "to,s,ss:sps: and:ss, orctions, with,0spspsercctions: orders,ders,<<andeing or,<<withs,eingcector:ctionsctions:<your: orctionsions:number:to, and,holderments orctions orhesisse orencespspshanmesps ordersmondgercectorion.<using,<withctor:toces,\n",
      "orgerswerswers orppings:<toderspondersions orctions,\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 'NoneType' object has no attribute 'group'\n",
      "\n",
      "You are a structured resume parsing agent.\n",
      "\n",
      "## INSTRUCTIONS:\n",
      "Extract the following fields from the resume. Respond only with a valid JSON object, using double quotes. Do not guess.\n",
      "\n",
      "- \"experience_years\": Float. Sum of relevant work experience in years. Use months/12 if stated. If unclear, use 0.0.\n",
      "- \"technical_skills\": List (max 10). Tools, languages, or frameworks. All lowercase.\n",
      "- \"soft_skills\": List (max 3). Interpersonal or cognitive skills. No technical terms. All lowercase.\n",
      "- \"education_level\": One of [\"bachelor\", \"master\", \"phd\", \"other\"].\n",
      "- \"fit_score\": Float [0.0–5.0], based on:\n",
      "  - experience (0–2)\n",
      "  - technical skill breadth (0–2)\n",
      "  - soft skills (0–1)\n",
      "\n",
      "## EXAMPLE RESUME:\n",
      "education: b.e in computer engineering, 2017. skills: python, java, sql, scikit-learn. worked 18 months as data analyst at ABC corp. known for team collaboration and adaptability.\n",
      "\n",
      "## EXAMPLE OUTPUT:\n",
      "{\n",
      "  \"experience_years\": 1.5,\n",
      "  \"technical_skills\": [\"python\", \"java\", \"sql\", \"scikit-learn\"],\n",
      "  \"soft_skills\": [\"collaboration\", \"adaptability\"],\n",
      "  \"education_level\": \"bachelor\",\n",
      "  \"fit_score\": 3.7\n",
      "}\n",
      "\n",
      "## RESUME:\n",
      "\"\"\"\n",
      "skills * programming languages: python (pandas, numpy, scipy, scikit-learn, matplotlib), sql, java, javascript/jquery. * machine learning: regression, svm, na ve bayes, knn, random forest, decision trees, boosting techniques, cluster analysis, word embedding, sentiment analysis, natural language processing, dimensionality reduction, topic modelling (lda, nmf), pca & neural nets. * database visualizations: mysql, sqlserver, cassandra, hbase, elasticsearch d3.js, dc.js, plotly, kibana, matplotlib, ggplot, tableau. * others: regular expression, html, css, angular 6, logstash, kafka, python flask, git, docker, computer vision - open cv and understanding of deep learning.education details data science assurance associate data science assurance associate - ernst & young llp skill details javascript- exprience - 24 months jquery- exprience - 24 months python- exprience - 24 monthscompany details company - ernst & young llp description - fraud investigations and dispute services assurance technology assisted review tar (technology assisted review) assists in accelerating the review process and run analytics and generate reports. * core member of a team helped in developing automated review platform tool from scratch for assisting e discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review. * understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. worked on analyzing the outputs and precision monitoring for the entire tool. * tar assists in predictive coding, topic modelling from the evidence by following ey standards. developed the classifier models in order to identify \"red flags\" and fraud-related issues. tools & technologies: python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, na ve bayes, lda, nmf for topic modelling, vader and text blob for sentiment analysis. matplot lib, tableau dashboard for reporting. multiple data science and analytic projects (usa clients) text analytics - motor vehicle customer review data * received customer feedback survey data for past one year. performed sentiment (positive, negative & neutral) and time series analysis on customer comments across all 4 categories. * created heat map of terms by survey category based on frequency of words * extracted positive and negative words across all the survey categories and plotted word cloud. * created customized tableau dashboards for effective reporting and visualizations. chatbot * developed a user friendly chatbot for one of our products which handle simple questions about hours of operation, reservation options and so on. * this chat bot serves entire product related questions. giving overview of tool via qa platform and also give recommendation responses so that user question to build chain of relevant answer. * this too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions. tools & technologies: python, natural language processing, nltk, spacy, topic modelling, sentiment analysis, word embedding, scikit-learn, javascript/jquery, sqlserver information governance organizations to make informed decisions about all of the information they store. the integrated information governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk. * scan data from multiple sources of formats and parse different file formats, extract meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana. * preforming rot analysis on the data which give information of data which helps identify content that is either redundant, outdated, or trivial. * preforming full-text search analysis on elastic search with predefined methods which can tag as (pii) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks. tools & technologies: python, flask, elastic search, kibana fraud analytic platform fraud analytics and investigative platform to review all red flag cases. fap is a fraud analytics and investigative platform with inbuilt case manager and suite of analytics for various erp systems. * it can be used by clients to interrogate their accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics tools & technologies: html, javascript, sqlserver, jquery, css, bootstrap, node.js, d3.js, dc.js\n",
      "\"\"\"\n",
      "\n",
      "## OUTPUT:\n",
      "{\n",
      "  \"experience_years\": 1.5,\n",
      "  \"technical_skills\": [\"python\", \"java\", \"sql\", \"scikit-learn\"],\n",
      "  \"soft_skills\": [\"collaboration\", \"adaptability\"],\n",
      "  \"education_level\": \"bachelor\",\n",
      "  \"fit_score\": 3.7,\n",
      "  \"skills\": {\n",
      "    \"python\": {\n",
      "      \"version\": \"3.7\",\n",
      "      \"interpreter\": \"python3\",\n",
      "      \"packages\": [\"numpy\", \"scipy\", \"scikit-learn\", \"matplotlib\", \"pandas\", \"seaborn\", \"plotly\", \"scikit-image\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"scikit-learn\", \"\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m resume \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i,::\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_prompt(resume[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Resume\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## OUTPUT:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m})\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:302\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\pipelines\\base.py:1431\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1425\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         )\n\u001b[0;32m   1429\u001b[0m     )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\pipelines\\base.py:1438\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1437\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1438\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\pipelines\\base.py:1338\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1337\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1338\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1339\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:400\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    398\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 400\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[0;32m    403\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2594\u001b[0m     )\n\u001b[0;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2598\u001b[0m         input_ids,\n\u001b[0;32m   2599\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2600\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2601\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2605\u001b[0m     )\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\generation\\utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3564\u001b[0m     outputs,\n\u001b[0;32m   3565\u001b[0m     model_kwargs,\n\u001b[0;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3567\u001b[0m )\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    685\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    689\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    690\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    691\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    692\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    693\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    694\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    695\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    696\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    697\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    699\u001b[0m )\n\u001b[0;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    454\u001b[0m     hidden_states,\n\u001b[0;32m    455\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    456\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    457\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    458\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    459\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    460\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    461\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    463\u001b[0m )\n\u001b[0;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Python\\python3.10\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:328\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    329\u001b[0m     outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (self_attn_weights,)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists('cleaned_resume_dataset.csv'):\n",
    "    os.remove('cleaned_resume_dataset.csv')\n",
    "\n",
    "    with open('cleaned_resume_dataset.csv', \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        head = [*df.columns[::2], *list(json_string.keys())]\n",
    "        writer.writerow(head)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    resume = df.iloc[i,::2]\n",
    "    prompt = build_prompt(resume['Cleaned_Resume'])\n",
    "\n",
    "    output = pipe(prompt, max_new_tokens=500, temperature=0.2)[0][\"generated_text\"]\n",
    "\n",
    "    try:\n",
    "        pattern = r\"## OUTPUT:\\s*(\\{.*\\})\"\n",
    "        match = re.search(pattern, output, re.DOTALL)\n",
    "               \n",
    "        json_text = match.group(1).strip()\n",
    "        json_data = json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR {e}\\n{output}\\n{'-'*30}\")\n",
    "        continue\n",
    "\n",
    "    with open('cleaned_resume_dataset.csv', \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        row = [*resume.tolist(), *list(json_data.values())]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f80665-d253-486e-8a3f-c3b703d1012a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
